{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa75abbe1d7b92be",
   "metadata": {},
   "source": [
    "下面是对使用LangChain进行信息提取链的步骤逐步解释，每个步骤后都有详细的说明和必要的类比，帮助更好地理解。\n",
    "\n",
    "### 1. 设置环境\n",
    "\n",
    "#### 1.1 安装LangChain\n",
    "你需要先安装LangChain库。可以使用以下命令通过pip或conda进行安装：\n",
    "```bash\n",
    "pip install langchain\n",
    "```\n",
    "或者\n",
    "```bash\n",
    "conda install langchain -c conda-forge\n",
    "```\n",
    "\n",
    "这种安装步骤类似于你在厨房准备做饭，首先需要确保所有的工具和食材都准备好。\n",
    "\n",
    "### 2. 定义Schema（模式）\n",
    "\n",
    "#### 2.1 使用Pydantic定义Schema\n",
    "你需要定义一个模式（schema），描述你想从文本中提取的信息。这就像设计一个模具，告诉机器要提取哪些具体的信息。以下是一个示例，用于提取个人信息：\n",
    "```python\n",
    "from typing import Optional\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class Person(BaseModel):\n",
    "    name: Optional[str] = Field(default=None, description=\"The name of the person\")\n",
    "    hair_color: Optional[str] = Field(default=None, description=\"The color of the person's hair\")\n",
    "    height_in_meters: Optional[str] = Field(default=None, description=\"Height measured in meters\")\n",
    "```\n",
    "\n",
    "这里的`Optional`表示这个字段是可选的，这样模型在没有足够信息时可以返回`None`，而不是编造一个值。\n",
    "\n",
    "一种容易理解的方式描述，也许可以使用日常类比：\n",
    "想象一下你在填写一个表格，这个表格有多个字段，例如名字、发色和身高。如果你不知道某个字段的信息，你可以留空。这里的模式就像是这个表格的模板。\n",
    "\n",
    "### 3. 创建提取器\n",
    "\n",
    "#### 3.1 定义提示模板（Prompt Template）\n",
    "提示模板告诉语言模型如何提取信息。我们使用以下代码来定义一个简单的提示模板：\n",
    "```python\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert extraction algorithm. Extract relevant information from the text. If an attribute is unknown, return null.\"),\n",
    "    (\"human\", \"{text}\")\n",
    "])\n",
    "```\n",
    "\n",
    "这种提示模板类似于给机器下达一个明确的任务指令：你是一个信息提取专家，从文本中提取相关信息，如果某个属性未知，就返回空值。\n",
    "\n",
    "一种容易理解的方式描述，也许可以使用日常类比：\n",
    "想象你是一个调查员，老板告诉你：“如果你在调查中遇到某些信息，不确定的话就不要乱填，直接标注为空。”\n",
    "\n",
    "#### 3.2 创建可运行对象（Runnable）\n",
    "使用上面的提示模板和语言模型来创建一个可运行的提取器：\n",
    "```python\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "runnable = prompt | llm.with_structured_output(schema=Person)\n",
    "```\n",
    "\n",
    "这里的`runnable`表示一个可以运行的链条，结合了提示模板和语言模型。\n",
    "\n",
    "### 4. 调用链条\n",
    "\n",
    "#### 4.1 测试提取链条\n",
    "提供一些文本并调用链条进行测试：\n",
    "```python\n",
    "text = \"Alan Smith is 6 feet tall and has blond hair.\"\n",
    "result = runnable.invoke({\"text\": text})\n",
    "print(result)\n",
    "# Output: Person(name='Alan Smith', hair_color='blond', height_in_meters='1.8288')\n",
    "```\n",
    "\n",
    "这里我们输入了一些文本，并使用我们定义的链条进行信息提取。结果是一个`Person`对象，包含提取的信息。\n",
    "\n",
    "一种容易理解的方式描述，也许可以使用日常类比：\n",
    "这就像是你向一个智能助手描述一个人，并让它告诉你这个人的详细信息。\n",
    "\n",
    "### 5. 提取多个实体\n",
    "\n",
    "#### 5.1 定义嵌套模式\n",
    "为了提取多个实体，可以使用嵌套模型。以下是一个示例：\n",
    "```python\n",
    "from typing import List\n",
    "\n",
    "class Data(BaseModel):\n",
    "    people: List[Person]\n",
    "\n",
    "runnable = prompt | llm.with_structured_output(schema=Data)\n",
    "text = \"Jeff has black hair and is 6 feet tall. Anna has the same hair color.\"\n",
    "result = runnable.invoke({\"text\": text})\n",
    "print(result)\n",
    "# Output: Data(people=[Person(name='Jeff', hair_color='black', height_in_meters='1.8288'), Person(name='Anna', hair_color='black', height_in_meters=None)])\n",
    "```\n",
    "\n",
    "这种嵌套模式允许提取多个实体的信息。\n",
    "\n",
    "一种容易理解的方式描述，也许可以使用日常类比：\n",
    "想象你在一个派对上遇到几个人，记下了每个人的名字和一些信息，现在你需要把这些信息整理成一个列表。\n",
    "\n",
    "### 6. 提高性能\n",
    "\n",
    "#### 6.1 使用参考示例\n",
    "通过使用参考示例和最佳实践，可以提高提取质量。提供详细的文档和可选属性，以防止模型生成错误信息。\n",
    "\n",
    "这种提高性能的步骤类似于在调查前先进行培训，确保调查员知道如何处理各种情况，以提高信息的准确性。\n",
    "\n",
    "通过这些步骤，你可以使用LangChain构建一个强大的信息提取链条，自动从文本中提取结构化信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54e043ad640996fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T14:16:31.861934Z",
     "start_time": "2024-07-13T14:16:28.219644Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][2024-07-13 22:16:29.629] oauth.py:228 [t:37696]: trying to refresh access_token for ak `l9aif3***`\n",
      "[INFO][2024-07-13 22:16:30.162] oauth.py:243 [t:37696]: sucessfully refresh access_token\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "您好！您想让我做什么呢？我很乐意帮助您。请告诉我您的需求，我会尽力回答您的问题。\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 62\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01muvicorn\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m     uvicorn\u001b[38;5;241m.\u001b[39mrun(app, host\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocalhost\u001b[39m\u001b[38;5;124m\"\u001b[39m, port\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1234\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\uvicorn\\main.py:577\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(app, host, port, uds, fd, loop, http, ws, ws_max_size, ws_max_queue, ws_ping_interval, ws_ping_timeout, ws_per_message_deflate, lifespan, interface, reload, reload_dirs, reload_includes, reload_excludes, reload_delay, workers, env_file, log_config, log_level, access_log, proxy_headers, server_header, date_header, forwarded_allow_ips, root_path, limit_concurrency, backlog, limit_max_requests, timeout_keep_alive, timeout_graceful_shutdown, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_version, ssl_cert_reqs, ssl_ca_certs, ssl_ciphers, headers, use_colors, app_dir, factory, h11_max_incomplete_event_size)\u001b[0m\n\u001b[0;32m    575\u001b[0m         Multiprocess(config, target\u001b[38;5;241m=\u001b[39mserver\u001b[38;5;241m.\u001b[39mrun, sockets\u001b[38;5;241m=\u001b[39m[sock])\u001b[38;5;241m.\u001b[39mrun()\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 577\u001b[0m         server\u001b[38;5;241m.\u001b[39mrun()\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    579\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39muds \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(config\u001b[38;5;241m.\u001b[39muds):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\uvicorn\\server.py:65\u001b[0m, in \u001b[0;36mServer.run\u001b[1;34m(self, sockets)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, sockets: \u001b[38;5;28mlist\u001b[39m[socket\u001b[38;5;241m.\u001b[39msocket] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39msetup_event_loop()\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserve(sockets\u001b[38;5;241m=\u001b[39msockets))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\asyncio\\runners.py:186\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug\u001b[38;5;241m=\u001b[39mdebug) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(main)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "from typing import List\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langserve import add_routes\n",
    "\n",
    "# 1. Create prompt template\n",
    "system_template = \"Take a deep breath and let's work it out step by step to make sure we get the right answer.  If there's a perfect solution, I tip $200!，Translate the following into {language}:\"\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")\n",
    "\n",
    "# 2. Create model\n",
    "\"\"\"For basic init and call\"\"\"\n",
    "import os\n",
    "\n",
    "from langchain_community.llms import QianfanLLMEndpoint\n",
    "\n",
    "# Set environment variables for authentication\n",
    "os.environ[\"QIANFAN_AK\"] = \"l9aif31SHOoS9qcQIF3zEevQ\"\n",
    "os.environ[\"QIANFAN_SK\"] = \"2YURLkXroE5VMZZOuyb7YXYNx7eoE4Yy\"\n",
    "\n",
    "\n",
    "\n",
    "model = QianfanLLMEndpoint(\n",
    "    streaming=True,\n",
    "    model=\"Yi-34B-Chat\",\n",
    "    endpoint=\"eb-instant\",\n",
    ")\n",
    "res = model.invoke(\"hi\")\n",
    "\n",
    "print(res)\n",
    "\n",
    "# 3. Create parser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# 4. Create chain\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "\n",
    "# 4. App definition\n",
    "app = FastAPI(\n",
    "  title=\"LangChain Server\",\n",
    "  version=\"1.0\",\n",
    "  description=\"A simple API server using LangChain's Runnable interfaces\",\n",
    ")\n",
    "\n",
    "# 5. Adding chain route\n",
    "\n",
    "add_routes(\n",
    "    app,\n",
    "    chain,\n",
    "    path=\"/chain\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "\n",
    "    uvicorn.run(app, host=\"localhost\", port=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74105f0d0d52c42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
